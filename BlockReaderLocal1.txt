Index: src/test/org/apache/hadoop/hdfs/TestFileLocalRead.java
===================================================================
--- src/test/org/apache/hadoop/hdfs/TestFileLocalRead.java	(revision 0)
+++ src/test/org/apache/hadoop/hdfs/TestFileLocalRead.java	(revision 0)
@@ -0,0 +1,207 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hdfs;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.FileReader;
+import java.io.IOException;
+import java.net.InetSocketAddress;
+
+import org.apache.commons.logging.impl.Log4JLogger;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.BlockLocation;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FSDataOutputStream;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hdfs.protocol.Block;
+import org.apache.hadoop.hdfs.protocol.DatanodeInfo;
+import org.apache.hadoop.hdfs.protocol.FSConstants;
+import org.apache.hadoop.hdfs.protocol.LocatedBlock;
+import org.apache.hadoop.hdfs.protocol.LocatedBlocks;
+import org.apache.hadoop.hdfs.server.datanode.DataNode;
+import org.apache.hadoop.hdfs.server.datanode.FSDataset;
+import org.apache.hadoop.hdfs.server.datanode.SimulatedFSDataset;
+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;
+import org.apache.hadoop.hdfs.server.namenode.LeaseManager;
+import org.apache.hadoop.io.IOUtils;
+import org.apache.log4j.Level;
+
+
+/**
+ * This class tests that a file need not be closed before its
+ * data can be read by another client.
+ */
+public class TestFileLocalRead extends junit.framework.TestCase {
+  static final String DIR = "/" + TestFileLocalRead.class.getSimpleName() + "/";
+
+  {
+    //((Log4JLogger)DataNode.LOG).getLogger().setLevel(Level.ALL);
+    ((Log4JLogger)LeaseManager.LOG).getLogger().setLevel(Level.ALL);
+    ((Log4JLogger)FSNamesystem.LOG).getLogger().setLevel(Level.ALL);
+    ((Log4JLogger)DFSClient.LOG).getLogger().setLevel(Level.ALL);
+  }
+
+  static final long seed = 0xDEADBEEFL;
+  static final int blockSize = 5120;
+  static final int numBlocks = 3;
+  static final int fileSize = numBlocks * blockSize + 100;
+  boolean simulatedStorage = false;
+
+  // creates a file but does not close it
+  static FSDataOutputStream createFile(FileSystem fileSys, Path name, int repl)
+    throws IOException {
+    System.out.println("createFile: Created " + name + " with " + repl + " replica.");
+    FSDataOutputStream stm = fileSys.create(name, true,
+                                            fileSys.getConf().getInt("io.file.buffer.size", 4096),
+                                            (short)repl, (long)blockSize);
+    return stm;
+  }
+
+  //
+  // writes to file but does not close it
+  //
+  static void writeFile(FSDataOutputStream stm) throws IOException {
+    writeFile(stm, fileSize);
+  }
+
+  //
+  // writes specified bytes to file.
+  //
+  static void writeFile(FSDataOutputStream stm, int size) throws IOException {
+    byte[] buffer = AppendTestUtil.randomBytes(seed, size);
+    stm.write(buffer, 0, size);
+  }
+
+  //
+  // verify that the data written to the full blocks are sane
+  // 
+  private void checkFile(FileSystem fileSys, Path name, int repl)
+    throws IOException {
+    boolean done = false;
+
+    // wait till all full blocks are confirmed by the datanodes.
+    while (!done) {
+      try {
+        Thread.sleep(1000);
+      } catch (InterruptedException e) {}
+      done = true;
+      BlockLocation[] locations = fileSys.getFileBlockLocations(
+          fileSys.getFileStatus(name), 0, fileSize);
+      if (locations.length < numBlocks) {
+        done = false;
+        continue;
+      }
+      for (int idx = 0; idx < locations.length; idx++) {
+        if (locations[idx].getHosts().length < repl) {
+          done = false;
+          break;
+        }
+      }
+    }
+    FSDataInputStream stm = fileSys.open(name);
+    final byte[] expected;
+    if (simulatedStorage) {
+      expected = new byte[numBlocks * blockSize];
+      for (int i= 0; i < expected.length; i++) {  
+        expected[i] = SimulatedFSDataset.DEFAULT_DATABYTE;
+      }
+    } else {
+      expected = AppendTestUtil.randomBytes(seed, numBlocks*blockSize);
+    }
+    // do a sanity check. Read the file
+    byte[] actual = new byte[numBlocks * blockSize];
+    System.out.println("Verifying file ");
+    stm.readFully(0, actual);
+    stm.close();
+    checkData(actual, 0, expected, "Read 1");
+  }
+
+  static private void checkData(byte[] actual, int from, byte[] expected, String message) {
+    for (int idx = 0; idx < actual.length; idx++) {
+      assertEquals(message+" byte "+(from+idx)+" differs. expected "+
+                   expected[from+idx]+" actual "+actual[idx],
+                   expected[from+idx], actual[idx]);
+      actual[idx] = 0;
+    }
+  }
+
+  static void checkFullFile(FileSystem fs, Path name) throws IOException {
+    FileStatus stat = fs.getFileStatus(name);
+    BlockLocation[] locations = fs.getFileBlockLocations(stat, 0, 
+                                                         fileSize);
+    for (int idx = 0; idx < locations.length; idx++) {
+      String[] hosts = locations[idx].getNames();
+      for (int i = 0; i < hosts.length; i++) {
+        System.out.print( hosts[i] + " ");
+      }
+      System.out.println(" off " + locations[idx].getOffset() +
+                         " len " + locations[idx].getLength());
+    }
+
+    byte[] expected = AppendTestUtil.randomBytes(seed, fileSize);
+    FSDataInputStream stm = fs.open(name);
+    byte[] actual = new byte[fileSize];
+    stm.readFully(0, actual);
+    checkData(actual, 0, expected, "Read 2");
+    stm.close();
+  }
+
+  /**
+   * Test that file data can be read by reading the block file
+   * directly from the local store.
+   */
+  public void testFileLocalRead() throws IOException {
+    Configuration conf = new Configuration();
+    conf.setBoolean("dfs.read.shortcircuit", true);
+    if (simulatedStorage) {
+      conf.setBoolean(SimulatedFSDataset.CONFIG_PROPERTY_SIMULATED, true);
+    }
+    MiniDFSCluster cluster = new MiniDFSCluster(conf, 1, true, null);
+    FileSystem fs = cluster.getFileSystem();
+    try {
+
+      //
+      // check that / exists
+      //
+      Path path = new Path("/");
+      System.out.println("Path : \"" + path.toString() + "\"");
+      System.out.println(fs.getFileStatus(path).isDir()); 
+      assertTrue("/ should be a directory", 
+                 fs.getFileStatus(path).isDir() == true);
+
+      // 
+      // create a new file in home directory. Do not close it.
+      //
+      Path file1 = new Path("filelocal.dat");
+      FSDataOutputStream stm = createFile(fs, file1, 1);
+
+      // write to file
+      writeFile(stm);
+      stm.close();
+
+      // Make sure a client can read it before it is closed.
+      checkFile(fs, file1, 1);
+
+    } finally {
+      cluster.shutdown();
+    }
+  }
+}
Index: src/test/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java
===================================================================
--- src/test/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java	(revision 11582)
+++ src/test/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java	(working copy)
@@ -20,6 +20,7 @@
 import java.io.IOException;
 import java.io.InputStream;
 import java.io.OutputStream;
+import java.io.File;
 import java.util.Arrays;
 import java.util.HashMap;
 import java.util.Random;
@@ -659,4 +660,8 @@
   public boolean hasEnoughResource() {
     return true;
   }
+
+  public File getBlockFile(Block blk) throws IOException {
+    throw new IOException("getBlockFile not supported.");
+  }
 }
Index: src/contrib/capacity-scheduler/src/test/org/apache/hadoop/mapred/TestCapacityScheduler.java
===================================================================
--- src/contrib/capacity-scheduler/src/test/org/apache/hadoop/mapred/TestCapacityScheduler.java	(revision 11582)
+++ src/contrib/capacity-scheduler/src/test/org/apache/hadoop/mapred/TestCapacityScheduler.java	(working copy)
@@ -377,7 +377,6 @@
       return true;
     }
     
-    @Override
     public boolean isRunning() {
       return !activeTasks.isEmpty();
     }
Index: src/hdfs/org/apache/hadoop/hdfs/DFSClient.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/DFSClient.java	(revision 11582)
+++ src/hdfs/org/apache/hadoop/hdfs/DFSClient.java	(working copy)
@@ -92,6 +92,8 @@
   private long namenodeVersion = ClientProtocol.versionID;
   private DFSClientMetrics metrics = new DFSClientMetrics();
   private Integer dataTransferVersion = -1;
+  private boolean shortCircuitLocalReads = false;
+  private final InetAddress localHost;
     
   /**
    * The locking hierarchy is to first acquire lock on DFSClient object, followed by
@@ -242,7 +244,8 @@
     this.maxBlockAcquireFailures = 
                           conf.getInt("dfs.client.max.block.acquire.failures",
                                       MAX_BLOCK_ACQUIRE_FAILURES);
-
+    this.localHost = InetAddress.getLocalHost();
+    
     // The hdfsTimeout is currently the same as the ipc timeout 
     this.hdfsTimeout = Client.getTimeout(conf);
 
@@ -273,6 +276,8 @@
           "Expecting exactly one of nameNodeAddr and rpcNamenode being null: "
           + "nameNodeAddr=" + nameNodeAddr + ", rpcNamenode=" + rpcNamenode);
     }
+    // read directly from the block file if configured.
+    this.shortCircuitLocalReads = conf.getBoolean("dfs.read.shortcircuit", true);
   }
 
   private void checkOpen() throws IOException {
@@ -1653,7 +1658,7 @@
       int nRead = super.read(buf, off, len);
       
       // if gotEOS was set in the previous read and checksum is enabled :
-      if (gotEOS && !eosBefore && nRead >= 0 && needChecksum()) {
+      if (dnSock != null && gotEOS && !eosBefore && nRead >= 0 && needChecksum()) {
         //checksum is verified and there are no errors.
         checksumOk(dnSock);
       }
@@ -1831,6 +1836,13 @@
       checksumSize = this.checksum.getChecksumSize();
     }
 
+    /**
+     * Public constructor
+     */
+    BlockReader(String file, int numRetries) {
+      super(new Path(file), numRetries);
+    }
+
     public static BlockReader newBlockReader(int dataTransferVersion,
         Socket sock, String file, long blockId, 
         long genStamp, long startOffset, long len, int bufferSize) throws IOException {
@@ -2137,11 +2149,36 @@
         chosenNode = retval.info;
         InetSocketAddress targetAddr = retval.addr;
 
+        // try reading the block locally. if this fails, then go via
+        // the datanode
+        Block blk = targetBlock.getBlock();
         try {
+          if (LOG.isDebugEnabled()) {
+            LOG.warn("blockSeekTo shortCircuitLocalReads " + shortCircuitLocalReads +
+                     " localhost " + localHost +
+                     " targetAddr " + targetAddr);
+          }
+          if (shortCircuitLocalReads && localHost != null && 
+              (targetAddr.equals(localHost) ||
+               targetAddr.getHostName().startsWith("localhost"))) {
+            blockReader = BlockReaderLocal.newBlockReader(conf, src, blk,
+                                                   chosenNode,
+                                                   offsetIntoBlock, 
+                                                   blk.getNumBytes() - offsetIntoBlock,
+                                                   metrics);
+            return chosenNode;
+          }
+        } catch (IOException ex) {
+          LOG.info("Failed to read block " + targetBlock.getBlock() +
+                   " on local machine " + localHost +
+                   ". Try via the datanode on " + targetAddr + ":" 
+                    + StringUtils.stringifyException(ex));
+        }
+
+        try {
           s = socketFactory.createSocket();
           NetUtils.connect(s, targetAddr, socketTimeout);
           s.setSoTimeout(socketTimeout);
-          Block blk = targetBlock.getBlock();
           
           blockReader = BlockReader.newBlockReader(
               getDataTransferProtocolVersion(),
@@ -2357,26 +2394,43 @@
         DatanodeInfo chosenNode = retval.info;
         InetSocketAddress targetAddr = retval.addr;
         BlockReader reader = null;
+        int len = (int) (end - start + 1);
             
-        try {
-          dn = socketFactory.createSocket();
-          NetUtils.connect(dn, targetAddr, socketTimeout);
-          dn.setSoTimeout(socketTimeout);
-              
-          int len = (int) (end - start + 1);
-              
-          reader = BlockReader.newBlockReader(getDataTransferProtocolVersion(),
+         try {
+           if (LOG.isDebugEnabled()) {
+             LOG.debug("fetchBlockByteRange shortCircuitLocalReads " + 
+                      shortCircuitLocalReads +
+                      " localhst " + localHost +
+                      " targetAddr " + targetAddr);
+           }
+           // first try reading the block locally.
+           if (shortCircuitLocalReads && localHost != null && 
+               (targetAddr.equals(localHost) || 
+                targetAddr.getHostName().startsWith("localhost"))) {
+             reader = BlockReaderLocal.newBlockReader(conf, src, 
+                                                  block.getBlock(),
+                                                  chosenNode,
+                                                  start, 
+                                                  len,
+                                                  metrics);
+            } else {
+              // go to the datanode
+              dn = socketFactory.createSocket();
+              NetUtils.connect(dn, targetAddr, socketTimeout);
+              dn.setSoTimeout(socketTimeout);
+              reader = BlockReader.newBlockReader(getDataTransferProtocolVersion(),
                                               dn, src, 
                                               block.getBlock().getBlockId(),
                                               block.getBlock().getGenerationStamp(),
                                               start, len, buffersize, 
                                               verifyChecksum, clientName);
-          int nread = reader.readAll(buf, offset, len);
-          if (nread != len) {
-            throw new IOException("truncated return from reader.read(): " +
-                                  "excpected " + len + ", got " + nread);
-          }
-          return;
+            }
+            int nread = reader.readAll(buf, offset, len);
+            if (nread != len) {
+              throw new IOException("truncated return from reader.read(): " +
+                                    "excpected " + len + ", got " + nread);
+            }
+            return;
         } catch (ChecksumException e) {
           ioe = e;
           LOG.warn("fetchBlockByteRange(). Got a checksum exception for " +
@@ -3500,7 +3554,8 @@
 
       } catch (IOException ie) {
 
-        LOG.info("Exception in createBlockOutputStream " + ie);
+        LOG.info("Exception in createBlockOutputStream " + nodes[0].getName() + " " +
+                 ie);
 
         // find the datanode that matches
         if (firstBadLink.length() != 0) {
Index: src/hdfs/org/apache/hadoop/hdfs/protocol/ClientDatanodeProtocol.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/protocol/ClientDatanodeProtocol.java	(revision 11582)
+++ src/hdfs/org/apache/hadoop/hdfs/protocol/ClientDatanodeProtocol.java	(working copy)
@@ -66,4 +66,10 @@
    */
   public void copyBlock(Block srcblock, Block destBlock,
       DatanodeInfo target) throws IOException;
+
+  /** Retrives the filename of the blockfile and the metafile from the datanode
+   * @param block the specified block on this datanode
+   * @return the BlockPathInfo of a block
+   */
+  BlockPathInfo getBlockPathInfo(Block block) throws IOException;
 }
Index: src/hdfs/org/apache/hadoop/hdfs/protocol/BlockPathInfo.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/protocol/BlockPathInfo.java	(revision 0)
+++ src/hdfs/org/apache/hadoop/hdfs/protocol/BlockPathInfo.java	(revision 0)
@@ -0,0 +1,64 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hdfs.protocol;
+
+import java.io.*;
+
+import org.apache.hadoop.io.*;
+
+/**
+ * Path information for a block. This includes the
+ * blocks metadata including the full pathname of the block on
+ * the local file system.
+ */
+public class BlockPathInfo extends Block {
+  static final WritableFactory FACTORY = new WritableFactory() {
+    public Writable newInstance() { return new BlockPathInfo(); }
+  };
+  static {                                      // register a ctor
+    WritableFactories.setFactory(BlockPathInfo.class, FACTORY);
+  }
+
+  private String localBlockPath = "";  // local file storing the data
+  private String localMetaPath = "";   // local file storing the checksum
+
+  public BlockPathInfo() {}
+
+  public BlockPathInfo(Block b, String file, String metafile) {
+    super(b);
+    this.localBlockPath = file;
+    this.localMetaPath = metafile;
+  }
+
+  public String getBlockPath() {return localBlockPath;}
+  public String getMetaPath() {return localMetaPath;}
+
+  /** {@inheritDoc} */
+  public void write(DataOutput out) throws IOException {
+    super.write(out);
+    Text.writeString(out, localBlockPath);
+    Text.writeString(out, localMetaPath);
+  }
+
+  /** {@inheritDoc} */
+  public void readFields(DataInput in) throws IOException {
+    super.readFields(in);
+    localBlockPath = Text.readString(in);
+    localMetaPath = Text.readString(in);
+  }
+}
Index: src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java	(revision 11582)
+++ src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java	(working copy)
@@ -23,6 +23,7 @@
 import java.io.IOException;
 import java.io.InputStream;
 import java.io.OutputStream;
+import java.io.File;
 
 
 
@@ -270,4 +271,9 @@
    * @return true if more then minimum valid volumes left in the FSDataSet
    */
   public boolean hasEnoughResource();
+
+  /**
+   * Get File name for a given data block.
+   **/
+  public File getBlockFile(Block b) throws IOException;
 }
Index: src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java	(revision 11582)
+++ src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java	(working copy)
@@ -48,6 +48,7 @@
 import org.apache.hadoop.conf.Configured;
 import org.apache.hadoop.hdfs.HDFSPolicyProvider;
 import org.apache.hadoop.hdfs.protocol.Block;
+import org.apache.hadoop.hdfs.protocol.BlockPathInfo;
 import org.apache.hadoop.hdfs.protocol.BlockListAsLongs;
 import org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol;
 import org.apache.hadoop.hdfs.protocol.DataTransferProtocol;
@@ -1540,6 +1541,20 @@
         + ": " + protocol);
   }
 
+  /** {@inheritDoc} */
+  public BlockPathInfo getBlockPathInfo(Block block) throws IOException {
+    File datafile = data.getBlockFile(block);
+    File metafile = FSDataset.getMetaFile(datafile, block);
+    BlockPathInfo info = new BlockPathInfo(block, datafile.getAbsolutePath(), 
+                                           metafile.getAbsolutePath());
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("getBlockPathInfo successful block=" + block +
+                " blockfile " + datafile.getAbsolutePath() +
+                " metafile " + metafile.getAbsolutePath());
+    }
+    return info;
+  }
+
   public ProtocolSignature getProtocolSignature(String protocol,
       long clientVersion, int clientMethodsHash) throws IOException {
     return ProtocolSignature.getProtocolSigature(
Index: src/hdfs/org/apache/hadoop/hdfs/metrics/DFSClientMetrics.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/metrics/DFSClientMetrics.java	(revision 11582)
+++ src/hdfs/org/apache/hadoop/hdfs/metrics/DFSClientMetrics.java	(working copy)
@@ -16,6 +16,10 @@
   public MetricsTimeVaryingRate lsLatency = new MetricsTimeVaryingRate(
       "client.ls.latency", registry,
       "The time taken by DFSClient to perform listStatus");
+  public MetricsTimeVaryingLong readsFromLocalFile = new MetricsTimeVaryingLong(
+      "client.read.localfile", registry,
+      "The number of time read is fetched directly from local file.");
+        
   private long numLsCalls = 0;
   private static Log log = LogFactory.getLog(DFSClientMetrics.class);
   final MetricsRecord metricsRecord;
Index: src/hdfs/org/apache/hadoop/hdfs/BlockReaderLocal.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/BlockReaderLocal.java	(revision 0)
+++ src/hdfs/org/apache/hadoop/hdfs/BlockReaderLocal.java	(revision 0)
@@ -0,0 +1,161 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hdfs;
+
+import org.apache.hadoop.io.*;
+import org.apache.hadoop.io.retry.RetryPolicies;
+import org.apache.hadoop.io.retry.RetryPolicy;
+import org.apache.hadoop.io.retry.RetryProxy;
+import org.apache.hadoop.fs.*;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.ipc.*;
+import org.apache.hadoop.net.NetUtils;
+import org.apache.hadoop.net.NodeBase;
+import org.apache.hadoop.conf.*;
+import org.apache.hadoop.hdfs.DistributedFileSystem.DiskStatus;
+import org.apache.hadoop.hdfs.protocol.*;
+import org.apache.hadoop.hdfs.server.common.HdfsConstants;
+import org.apache.hadoop.hdfs.server.common.UpgradeStatusReport;
+import org.apache.hadoop.hdfs.server.datanode.DataNode;
+import org.apache.hadoop.hdfs.server.namenode.NameNode;
+import org.apache.hadoop.hdfs.server.namenode.NotReplicatedYetException;
+import org.apache.hadoop.security.AccessControlException;
+import org.apache.hadoop.security.UnixUserGroupInformation;
+import org.apache.hadoop.util.*;
+import org.apache.hadoop.hdfs.DFSClient.BlockReader;
+import org.apache.hadoop.hdfs.metrics.DFSClientMetrics;
+
+import org.apache.commons.logging.*;
+
+import java.io.*;
+import java.net.*;
+import java.util.*;
+import java.util.zip.CRC32;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.ConcurrentHashMap;
+import java.nio.BufferOverflowException;
+import java.nio.ByteBuffer;
+
+import javax.net.SocketFactory;
+import javax.security.auth.login.LoginException;
+
+/** This is a local block reader. if the DFS client is on
+ * the same machine as the datanode, then the client can read
+ * files directly from the local file system rathen than going
+ * thorugh the datanode. This dramatically improves performance.
+ */
+public class BlockReaderLocal extends BlockReader {
+
+  public static final Log LOG = LogFactory.getLog(DFSClient.class);
+
+  private Configuration conf;
+  private long startOffset;
+  private long length;
+  private BlockPathInfo pathinfo;
+  private FileInputStream fin;  // reader for the data file
+  private DFSClientMetrics metrics;
+  
+  /**
+   * The only way this object can be instantiated.
+   */
+  public static BlockReader newBlockReader(Configuration conf,
+    String file, Block blk, DatanodeInfo node, 
+    long startOffset, long length, DFSClientMetrics metrics) throws IOException {
+
+    // make RPC to local datanode to find local pathnames of blocks
+    ClientDatanodeProtocol datanode = null;
+    try {
+      datanode = DFSClient.createClientDatanodeProtocolProxy(node, conf);
+      BlockPathInfo pathinfo = datanode.getBlockPathInfo(blk);
+      return new BlockReaderLocal(conf, file, blk, startOffset, length,
+                                  pathinfo, metrics);
+    } finally {
+      if (datanode != null) {
+        RPC.stopProxy(datanode);
+      }
+    }
+  }
+
+  private BlockReaderLocal(Configuration conf, String hdfsfile, Block blk,
+                          long startOffset, long length,
+                          BlockPathInfo pathinfo, DFSClientMetrics metrics) 
+                          throws IOException {
+    super(hdfsfile, 1);
+    this.pathinfo = pathinfo;
+    this.startOffset = startOffset;
+    this.length = length;    
+    this.metrics = metrics;
+
+    // get a local file system
+    File blkfile = new File(pathinfo.getBlockPath());
+    fin = new FileInputStream(blkfile);
+    fin.getChannel().position(startOffset);
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("BlockChecksumFileSystem constructor for file " +
+                blkfile + " of size " + blkfile.length() +
+                " startOffset " + startOffset +
+                " length " + length);
+    }
+  }
+
+  @Override
+  public synchronized int read(byte[] buf, int off, int len)
+                               throws IOException {
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("BlockChecksumFileSystem read off " + off + " len " + len);
+    }
+    metrics.readsFromLocalFile.inc();
+    return fin.read(buf, off, len);
+  }
+
+  @Override
+  public synchronized long skip(long n) throws IOException {
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("BlockChecksumFileSystem skip " + n);
+    }
+    return fin.skip(n);
+  }
+
+  @Override
+  public synchronized void seek(long n) throws IOException {
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("BlockChecksumFileSystem seek " + n);
+    }
+    throw new IOException("Seek() is not supported in BlockReaderLocal");
+  }
+
+  @Override
+  protected synchronized int readChunk(long pos, byte[] buf, int offset,
+                                       int len, byte[] checksumBuf)
+                                       throws IOException {
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("BlockChecksumFileSystem readChunk pos " + pos +
+                " offset " + offset + " len " + len);
+    }
+    throw new IOException("readChunk() is not supported in BlockReaderLocal");
+  }
+
+  @Override
+  public synchronized void close() throws IOException {
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("BlockChecksumFileSystem close");
+    }
+    fin.close();
+  }
+}
+
